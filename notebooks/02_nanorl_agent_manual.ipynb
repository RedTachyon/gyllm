{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ceb8391",
   "metadata": {},
   "source": [
    "# NanoRL agent manual stepping\n",
    "\n",
    "This notebook shows how to build a NanoRL agent and step it through an env\n",
    "with explicit AgentState handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c33ba",
   "metadata": {},
   "source": [
    "## Notes\n",
    "These examples load a real LLM. Run them in a GPU-enabled environment (e.g., a uv venv with CUDA/vllm installed or a container).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33891b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import SamplingParams\n",
    "\n",
    "import gyllm\n",
    "from nanorl.agent import InstructAgent\n",
    "from nanorl.rollout import NanoLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a4848",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25c93bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=\"bfloat16\",\n",
    "    device_map=\"cuda\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8193c",
   "metadata": {},
   "source": [
    "## Create a NanoLLM wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ada4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 23:11:59 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 01-11 23:12:00 [utils.py:233] non-default args: {'tokenizer': '/tmp/nanollm_tokenizer_p9usoozb', 'load_format': 'dummy', 'dtype': torch.bfloat16, 'distributed_executor_backend': 'uni', 'gpu_memory_utilization': 0.4, 'disable_log_stats': True, 'enforce_eager': True, 'enable_sleep_mode': True, 'model_impl': 'transformers', 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}\n",
      "WARNING 01-11 23:12:00 [model.py:371] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n",
      "INFO 01-11 23:12:05 [model.py:547] Resolved architecture: TransformersForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 23:12:05 [model.py:1510] Using max model len 32768\n",
      "INFO 01-11 23:12:07 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 01-11 23:12:07 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 01-11 23:12:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0+582e4e37.nv25.11) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='/tmp/nanollm_tokenizer_p9usoozb', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=dummy, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 01-11 23:12:16 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 01-11 23:12:17 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 01-11 23:12:17 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n",
      "INFO 01-11 23:12:17 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "INFO 01-11 23:12:17 [transformers.py:442] Using Transformers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 23:12:17 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "INFO 01-11 23:12:17 [gpu_model_runner.py:2653] Model loading took 0.9249 GiB and 0.147455 seconds\n",
      "INFO 01-11 23:12:43 [gpu_worker.py:298] Available KV cache memory: 45.28 GiB\n",
      "INFO 01-11 23:12:43 [kv_cache_utils.py:1087] GPU KV cache size: 3,956,720 tokens\n",
      "INFO 01-11 23:12:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 120.75x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 23:12:45,179 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2026-01-11 23:12:45,383 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-11 23:12:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "INFO 01-11 23:12:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 27.69 seconds\n",
      "INFO 01-11 23:12:45 [llm.py:306] Supported_tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "llm = NanoLLM(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    gpu_memory_utilization=0.4,\n",
    "    enable_sleep_mode=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac207a",
   "metadata": {},
   "source": [
    "## Create an environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62b4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gyllm.make(\"simple/reverse_echo\", env_kwargs={\"num_turns\": 2})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3071c",
   "metadata": {},
   "source": [
    "## Build an agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c776b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=64)\n",
    "agent = InstructAgent(\n",
    "    model=model,\n",
    "    llm=llm,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling_params=sampling_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d7e55",
   "metadata": {},
   "source": [
    "## Step through the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f77ae0f-2afe-4cd2-96e1-41baee1f5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actor': 'agent',\n",
       "  'reward': 0.0,\n",
       "  'system_message': {'role': 'system',\n",
       "   'content': 'You are in ReverseEcho.\\nEach turn, the environment will send you a message.\\nYour task is to reply with the exact same message.\\nDo not add extra words.\\nLeading/trailing whitespace is ignored.\\n'},\n",
       "  'message': {'role': 'user', 'content': 'harbor'},\n",
       "  'needs_action': True,\n",
       "  'info': {'turn': 0, 'num_turns': 2},\n",
       "  'episode_id': 2,\n",
       "  'episode_start': True,\n",
       "  'episode_end': False}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests = env.reset()\n",
    "state = None\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3264a0-7bbf-4ee5-9fa9-8fad047e9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actor': 'agent',\n",
       "  'reward': 1.0,\n",
       "  'message': {'role': 'user', 'content': 'delta'},\n",
       "  'needs_action': True,\n",
       "  'info': {'turn': 1, 'num_turns': 2},\n",
       "  'episode_id': 2,\n",
       "  'episode_start': False,\n",
       "  'episode_end': False}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions, state = agent.act(requests, state)\n",
    "requests = env.step(actions)\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcf5ce9-3b2d-49b7-8e0d-ec5d71c0c421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actor': 'agent',\n",
       "  'reward': 1.0,\n",
       "  'message': {'role': 'user', 'content': 'Done.'},\n",
       "  'needs_action': False,\n",
       "  'info': {'turn': 2, 'num_turns': 2},\n",
       "  'episode_id': 2,\n",
       "  'episode_start': False,\n",
       "  'episode_end': True}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions, state = agent.act(requests, state)\n",
    "requests = env.step(actions)\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df1d436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EpisodeRollout(actor='agent', messages=[{'role': 'system', 'content': 'You are in ReverseEcho.\\nEach turn, the environment will send you a message.\\nYour task is to reply with the exact same message.\\nDo not add extra words.\\nLeading/trailing whitespace is ignored.\\n'}, {'role': 'user', 'content': 'harbor'}, {'role': 'assistant', 'content': 'harbor'}, {'role': 'user', 'content': 'delta'}, {'role': 'assistant', 'content': 'delta'}, {'role': 'user', 'content': 'Done.'}], rewards=[1.0, 1.0], actions=['harbor', 'delta'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions, state = agent.act(requests, state)\n",
    "state.completed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
