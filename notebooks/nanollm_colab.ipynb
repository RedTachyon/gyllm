{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoLLM weight-sharing demo\n"
   ],
   "id": "27ac4c65cf69cf02"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip -q install vllm transformers\n"
   ],
   "id": "d202e4a4f81215f8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import Any\n",
    "os.environ.setdefault(\"VLLM_USE_V1\", \"1\")\n",
    "os.environ.setdefault(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"0\")\n",
    "os.environ[\"VLLM_NO_STDOUT_REDIRECT\"] = \"1\"\n",
    "try:\n",
    "    sys.stdout.fileno()\n",
    "except io.UnsupportedOperation:\n",
    "    sys.stdout.fileno = lambda: 1\n",
    "    sys.stderr.fileno = lambda: 2\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "def _rebind_padded_vocab_weight(vllm_weight: torch.Tensor, hf_weight_param: torch.nn.Parameter) -> None:\n",
    "    vocab = hf_weight_param.shape[0]\n",
    "    with torch.no_grad():\n",
    "        vllm_weight[:vocab].copy_(hf_weight_param.data)\n",
    "        if vllm_weight.shape[0] > vocab:\n",
    "            vllm_weight[vocab:].zero_()\n",
    "        hf_weight_param.data = vllm_weight[:vocab]\n",
    "def _match_param(name: str, hf_params: dict[str, torch.nn.Parameter]) -> torch.nn.Parameter | None:\n",
    "    if name in hf_params:\n",
    "        return hf_params[name]\n",
    "    if name.startswith(\"model.\") and name[len(\"model.\") :] in hf_params:\n",
    "        return hf_params[name[len(\"model.\") :]]\n",
    "    if name.startswith(\"transformer.\") and name[len(\"transformer.\") :] in hf_params:\n",
    "        return hf_params[name[len(\"transformer.\") :]]\n",
    "    return None\n",
    "def _bind_vllm_weights(vllm_model: Any, hf_model: Any) -> None:\n",
    "    tgt_param = next(iter(vllm_model.parameters()))\n",
    "    hf_model.to(device=tgt_param.device, dtype=tgt_param.dtype)\n",
    "    hf_model.eval()\n",
    "    vllm_in = vllm_model.model.get_input_embeddings()\n",
    "    hf_in = hf_model.get_input_embeddings()\n",
    "    vllm_in_w = vllm_in.weight\n",
    "    hf_in_w = hf_in.weight\n",
    "    if vllm_in_w.shape == hf_in_w.shape:\n",
    "        vllm_in_w.data = hf_in_w.data\n",
    "    else:\n",
    "        _rebind_padded_vocab_weight(vllm_in_w, hf_in_w)\n",
    "    vllm_head_w = vllm_model.lm_head.weight\n",
    "    hf_out_w = hf_model.get_output_embeddings().weight\n",
    "    if vllm_head_w.shape == hf_out_w.shape:\n",
    "        vllm_head_w.data = hf_out_w.data\n",
    "    else:\n",
    "        _rebind_padded_vocab_weight(vllm_head_w, hf_out_w)\n",
    "    hf_params = dict(hf_model.named_parameters())\n",
    "    with torch.no_grad():\n",
    "        for name, p in vllm_model.named_parameters():\n",
    "            hf_p = _match_param(name, hf_params)\n",
    "            if hf_p is None or p.shape != hf_p.shape:\n",
    "                continue\n",
    "            p.data = hf_p.data\n",
    "class NanoLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_model: Any,\n",
    "        *,\n",
    "        tokenizer: str | Any | None = None,\n",
    "        model_id: str | None = None,\n",
    "        **vllm_kwargs: Any,\n",
    "    ) -> None:\n",
    "        self.hf_model = hf_model\n",
    "        resolved_model_id = model_id or getattr(hf_model, \"name_or_path\", None) or hf_model.config._name_or_path\n",
    "        if tokenizer is None:\n",
    "            tokenizer_path = resolved_model_id\n",
    "        elif isinstance(tokenizer, str):\n",
    "            tokenizer_path = tokenizer\n",
    "        else:\n",
    "            tmp_dir = tempfile.mkdtemp(prefix=\"nanollm_tokenizer_\")\n",
    "            tokenizer.save_pretrained(tmp_dir)\n",
    "            tokenizer_path = tmp_dir\n",
    "        vllm_kwargs.setdefault(\"distributed_executor_backend\", \"uni\")\n",
    "        vllm_kwargs.setdefault(\"tensor_parallel_size\", 1)\n",
    "        vllm_kwargs.setdefault(\"pipeline_parallel_size\", 1)\n",
    "        vllm_kwargs.setdefault(\"model_impl\", \"transformers\")\n",
    "        vllm_kwargs.setdefault(\"load_format\", \"dummy\")\n",
    "        vllm_kwargs.setdefault(\"enforce_eager\", True)\n",
    "        self._llm = LLM(model=resolved_model_id, tokenizer=tokenizer_path, **vllm_kwargs)\n",
    "        self.sync_weights()\n",
    "    @property\n",
    "    def llm(self) -> Any:\n",
    "        return self._llm\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        return getattr(self._llm, name)\n",
    "    def generate(self, prompts: str | list[str], sampling_params: SamplingParams, **kwargs: Any) -> Any:\n",
    "        return self._llm.generate(prompts, sampling_params, **kwargs)\n",
    "    def sync_weights(self) -> None:\n",
    "        self._llm.apply_model(lambda vllm_model: _bind_vllm_weights(vllm_model, self.hf_model))\n"
   ],
   "id": "b5fa50c1d68bee20"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, technical assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write one sentence about in-place weight sharing.\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "llm = NanoLLM(hf_model, tokenizer=tokenizer, model_id=model_id)\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=48)\n",
    "\n",
    "outputs = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "before = outputs[0].outputs[0].text.strip()\n",
    "print(\"=== before ===\")\n",
    "print(before)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param in hf_model.parameters():\n",
    "        if torch.is_floating_point(param):\n",
    "            param.zero_()\n",
    "\n",
    "outputs = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "after = outputs[0].outputs[0].text.strip()\n",
    "print(\"=== after ===\")\n",
    "print(after)\n"
   ],
   "id": "d08c5f976a160d1c"
  }
 ],
 "metadata": {
  "colab": {
   "name": "nanollm_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
