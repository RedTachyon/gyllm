{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tic-Tac-Toe REINFORCE (Colab)\n",
        "\n",
        "This notebook mirrors `scripts/ttt_reinforce.py` with smaller defaults for Colab.\n",
        "Use a GPU runtime (T4/A100) for vLLM + training.\n"
      ],
      "id": "90e82b58f5b6d865"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Clone and install. Replace RedTachyon with your GitHub org/user.\n",
        "!git clone https://github.com/RedTachyon/nanorl.git\n",
        "%cd nanorl\n",
        "!pip -q install -e packages/gyllm -e packages/nanorl\n"
      ],
      "id": "3fb243a2849977f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"VLLM_CONFIGURE_LOGGING\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from vllm import SamplingParams\n",
        "\n",
        "import gyllm\n",
        "from nanorl.rl import compute_reinforce_loss\n",
        "from nanorl.rollout import rollout_autoreset_batched\n",
        "from nanorl.rollout.reporting import summarize_rollouts\n",
        "from nanorl.rollout import NanoLLM\n",
        "from gyllm.envs import AutoResetWrapper\n"
      ],
      "id": "9bb67844febb83ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Small defaults for Colab. Adjust as needed.\n",
        "model_id = \"Qwen/Qwen3-0.6B\"\n",
        "num_envs = 2\n",
        "episodes = 4 * num_envs\n",
        "num_updates = 2\n",
        "lr = 1e-5\n",
        "max_grad_norm = 1.0\n",
        "minibatch_size = 2\n",
        "\n",
        "wandb.init(mode=\"disabled\")\n"
      ],
      "id": "ae21a6366076d20c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if device == \"cuda\" else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=dtype)\n",
        "model.to(device)\n",
        "device = next(model.parameters()).device\n",
        "\n",
        "llm = NanoLLM(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        "    gpu_memory_utilization=0.4,\n",
        "    enable_sleep_mode=True,\n",
        ")\n",
        "\n",
        "env = gyllm.make(\n",
        "    \"simple/tic_tac_toe\",\n",
        "    env_kwargs={\"opponent\": \"random\"},\n",
        "    num_envs=num_envs,\n",
        ")\n",
        "env = AutoResetWrapper(env)\n",
        "\n",
        "sampling_params = SamplingParams(temperature=1.0, max_tokens=20)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n"
      ],
      "id": "ca6334c8d33d1e26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for update in range(num_updates):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        llm.wake_up()\n",
        "        rollouts = rollout_autoreset_batched(\n",
        "            env,\n",
        "            llm,\n",
        "            tokenizer,\n",
        "            sampling_params,\n",
        "            max_episodes=episodes,\n",
        "            max_steps=None,\n",
        "        )\n",
        "    llm.sleep(1)\n",
        "\n",
        "    _tokens, mean_reward, _sample_text = summarize_rollouts(rollouts, tokenizer)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    total_rollouts = len(rollouts)\n",
        "    total_loss_value = 0.0\n",
        "    total_assistant_tokens = 0.0\n",
        "    total_logprob = 0.0\n",
        "    reward_sum = 0.0\n",
        "\n",
        "    for start in range(0, total_rollouts, minibatch_size):\n",
        "        minibatch = rollouts[start : start + minibatch_size]\n",
        "        loss, mb_metrics = compute_reinforce_loss(\n",
        "            minibatch,\n",
        "            model,\n",
        "            tokenizer,\n",
        "            device=device,\n",
        "        )\n",
        "        mb_size = len(minibatch)\n",
        "        reward_sum += mb_metrics[\"avg_reward\"] * mb_size\n",
        "        total_assistant_tokens += mb_metrics[\"assistant_tokens\"]\n",
        "        total_logprob += mb_metrics[\"avg_logprob\"] * mb_metrics[\"assistant_tokens\"]\n",
        "\n",
        "        if mb_metrics[\"assistant_tokens\"] <= 0:\n",
        "            continue\n",
        "\n",
        "        scale = mb_size / max(total_rollouts, 1)\n",
        "        (loss * scale).backward()\n",
        "        total_loss_value += float(loss.item()) * scale\n",
        "\n",
        "    avg_reward = reward_sum / max(total_rollouts, 1)\n",
        "    metrics = {\n",
        "        \"avg_reward\": avg_reward,\n",
        "        \"baseline\": avg_reward,\n",
        "        \"avg_logprob\": total_logprob / max(total_assistant_tokens, 1.0),\n",
        "        \"assistant_tokens\": float(total_assistant_tokens),\n",
        "    }\n",
        "\n",
        "    if metrics[\"assistant_tokens\"] > 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\n",
        "        f\"update={update} loss={total_loss_value:.4f} \"\n",
        "        f\"avg_reward={metrics['avg_reward']:.3f} \"\n",
        "        f\"assistant_tokens={metrics['assistant_tokens']:.0f}\"\n",
        "    )\n",
        "\n",
        "env.close()\n",
        "wandb.finish()\n"
      ],
      "id": "84c73042b1dade98"
    }
  ],
  "metadata": {
    "colab": {
      "name": "ttt_reinforce_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}